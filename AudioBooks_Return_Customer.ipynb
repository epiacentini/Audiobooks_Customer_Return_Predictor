{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries \n",
    "### Note: we are going to being using numpy and not pandas because it will make it easier to process the data in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data using numpy. We ignore the first column because it is just the IDs which are of no use to the algorithm. Split the inputs and the targets into different arrays (targets are the last column so this can easily be done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')\n",
    "\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_all[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We shuffle the data here because the data was collected sequentially (by date) and we want the entries to be spread out randomly. This will help us out later when we train the model in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(unscaled_inputs_all.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "unscaled_inputs_all = unscaled_inputs_all[shuffled_indices]\n",
    "targets_all = targets_all[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is important that our dataset is balanced. This involves having roughly an equal amount of each class. In this case an equal amount of entries where the target is 0 (not a return customer) as entries where the target is 1 (a return customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))\n",
    "zero_targets_counter = 0\n",
    "\n",
    "indices_to_remove = []\n",
    "\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We calculated the number of target values that equal one using np.sum(). Now we iterate through all the targets and keep track of the number of targest that have value 0. Once we have found the same amount of targets that equal 0 as targets that equal 1, we can discard the rest of the target values that equal 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the inputs using the preproccessing function imported from sklearn earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Shuffle the data, same way as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = shuffled_inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4474\n"
     ]
    }
   ],
   "source": [
    "print(samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before putting the data into the model we must split it into training,testing and validation data. While it varies case to case I am going with an 80/10/10 split (training, testing, validation). A simple way of doing this is just multiplying 0.8 by the total number of entries and using that number as the index of the array. Then continuing on and making the next index the sum of the previous index and the amount of new entries (for validation this would be the amount of training data + the number of validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can verify that the targets are roughly split up equally between values of 1 and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1794.0 3579 0.501257334450964\n",
      "214.0 447 0.47874720357941836\n",
      "229.0 448 0.5111607142857143\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tensorflow which we will use to make our Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model with an input value of 10 and an output size of 2 because there are only two possibilites 0 or 1. While the hidden layer size does not have to be the same throughout each layer I am going to make them all 50. Early stopping is a callback function passed to the model that regardless of how many epochs specified will tell the model to stop training if it notices the loss increase (the patience parameter is how many times it has to notice the loss increase before stopping).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "hidden_layer_size = 50\n",
    "output_size = 2\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation = 'relu'),\n",
    "                            tf.keras.layers.Dropout(0.2),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation = 'relu'),\n",
    "                            tf.keras.layers.Dropout(0.2),\n",
    "                            tf.keras.layers.Dense(output_size,activation = 'sigmoid'),\n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The activation functions in the hidden layers will use the relu. Whereas the on the output layer I chose to use a sigmoid function which has a prediction range between 0 and 1 which works well when working with a binary outcome (0 or 1). A softmax function would also work well in place of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/28 - 0s - loss: 0.6637 - accuracy: 0.6150 - val_loss: 0.6005 - val_accuracy: 0.7315\n",
      "Epoch 2/100\n",
      "28/28 - 0s - loss: 0.5674 - accuracy: 0.7390 - val_loss: 0.5051 - val_accuracy: 0.7964\n",
      "Epoch 3/100\n",
      "28/28 - 0s - loss: 0.4869 - accuracy: 0.7608 - val_loss: 0.4419 - val_accuracy: 0.8076\n",
      "Epoch 4/100\n",
      "28/28 - 0s - loss: 0.4506 - accuracy: 0.7664 - val_loss: 0.4143 - val_accuracy: 0.8098\n",
      "Epoch 5/100\n",
      "28/28 - 0s - loss: 0.4327 - accuracy: 0.7734 - val_loss: 0.4019 - val_accuracy: 0.8054\n",
      "Epoch 6/100\n",
      "28/28 - 0s - loss: 0.4244 - accuracy: 0.7664 - val_loss: 0.3911 - val_accuracy: 0.8143\n",
      "Epoch 7/100\n",
      "28/28 - 0s - loss: 0.4126 - accuracy: 0.7776 - val_loss: 0.3900 - val_accuracy: 0.8076\n",
      "Epoch 8/100\n",
      "28/28 - 0s - loss: 0.4134 - accuracy: 0.7807 - val_loss: 0.3924 - val_accuracy: 0.8031\n",
      "Epoch 9/100\n",
      "28/28 - 0s - loss: 0.4038 - accuracy: 0.7913 - val_loss: 0.3820 - val_accuracy: 0.8121\n",
      "Epoch 10/100\n",
      "28/28 - 0s - loss: 0.3981 - accuracy: 0.7921 - val_loss: 0.3766 - val_accuracy: 0.8233\n",
      "Epoch 11/100\n",
      "28/28 - 0s - loss: 0.3970 - accuracy: 0.7809 - val_loss: 0.3747 - val_accuracy: 0.8210\n",
      "Epoch 12/100\n",
      "28/28 - 0s - loss: 0.3887 - accuracy: 0.7949 - val_loss: 0.3758 - val_accuracy: 0.8210\n",
      "Epoch 13/100\n",
      "28/28 - 0s - loss: 0.3878 - accuracy: 0.7960 - val_loss: 0.3691 - val_accuracy: 0.8277\n",
      "Epoch 14/100\n",
      "28/28 - 0s - loss: 0.3848 - accuracy: 0.7946 - val_loss: 0.3674 - val_accuracy: 0.8300\n",
      "Epoch 15/100\n",
      "28/28 - 0s - loss: 0.3821 - accuracy: 0.7983 - val_loss: 0.3662 - val_accuracy: 0.8277\n",
      "Epoch 16/100\n",
      "28/28 - 0s - loss: 0.3833 - accuracy: 0.7955 - val_loss: 0.3656 - val_accuracy: 0.8277\n",
      "Epoch 17/100\n",
      "28/28 - 0s - loss: 0.3776 - accuracy: 0.7994 - val_loss: 0.3643 - val_accuracy: 0.8300\n",
      "Epoch 18/100\n",
      "28/28 - 0s - loss: 0.3820 - accuracy: 0.7972 - val_loss: 0.3637 - val_accuracy: 0.8255\n",
      "Epoch 19/100\n",
      "28/28 - 0s - loss: 0.3751 - accuracy: 0.7999 - val_loss: 0.3625 - val_accuracy: 0.8300\n",
      "Epoch 20/100\n",
      "28/28 - 0s - loss: 0.3732 - accuracy: 0.8055 - val_loss: 0.3669 - val_accuracy: 0.8210\n",
      "Epoch 21/100\n",
      "28/28 - 0s - loss: 0.3731 - accuracy: 0.7997 - val_loss: 0.3633 - val_accuracy: 0.8255\n",
      "Epoch 22/100\n",
      "28/28 - 0s - loss: 0.3663 - accuracy: 0.8061 - val_loss: 0.3576 - val_accuracy: 0.8255\n",
      "Epoch 23/100\n",
      "28/28 - 0s - loss: 0.3706 - accuracy: 0.8094 - val_loss: 0.3611 - val_accuracy: 0.8300\n",
      "Epoch 24/100\n",
      "28/28 - 0s - loss: 0.3678 - accuracy: 0.8114 - val_loss: 0.3584 - val_accuracy: 0.8277\n",
      "Epoch 25/100\n",
      "28/28 - 0s - loss: 0.3698 - accuracy: 0.8080 - val_loss: 0.3587 - val_accuracy: 0.8300\n",
      "Epoch 26/100\n",
      "28/28 - 0s - loss: 0.3641 - accuracy: 0.8061 - val_loss: 0.3569 - val_accuracy: 0.8188\n",
      "Epoch 27/100\n",
      "28/28 - 0s - loss: 0.3611 - accuracy: 0.8103 - val_loss: 0.3549 - val_accuracy: 0.8277\n",
      "Epoch 28/100\n",
      "28/28 - 0s - loss: 0.3664 - accuracy: 0.8050 - val_loss: 0.3539 - val_accuracy: 0.8322\n",
      "Epoch 29/100\n",
      "28/28 - 0s - loss: 0.3639 - accuracy: 0.8097 - val_loss: 0.3533 - val_accuracy: 0.8322\n",
      "Epoch 30/100\n",
      "28/28 - 0s - loss: 0.3576 - accuracy: 0.8161 - val_loss: 0.3510 - val_accuracy: 0.8300\n",
      "Epoch 31/100\n",
      "28/28 - 0s - loss: 0.3632 - accuracy: 0.8080 - val_loss: 0.3558 - val_accuracy: 0.8277\n",
      "Epoch 32/100\n",
      "28/28 - 0s - loss: 0.3601 - accuracy: 0.8106 - val_loss: 0.3526 - val_accuracy: 0.8255\n",
      "Epoch 33/100\n",
      "28/28 - 0s - loss: 0.3608 - accuracy: 0.8064 - val_loss: 0.3514 - val_accuracy: 0.8255\n",
      "Epoch 34/100\n",
      "28/28 - 0s - loss: 0.3644 - accuracy: 0.8044 - val_loss: 0.3522 - val_accuracy: 0.8322\n",
      "Epoch 35/100\n",
      "28/28 - 0s - loss: 0.3536 - accuracy: 0.8164 - val_loss: 0.3516 - val_accuracy: 0.8345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fac7441f310>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 100\n",
    "\n",
    "model.fit(train_inputs,train_targets,batch_size = BATCH_SIZE, epochs = MAX_EPOCHS,callbacks=[early_stopping], validation_data = (validation_inputs, validation_targets), verbose = 2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.85      0.79       219\n",
      "         1.0       0.83      0.71      0.76       229\n",
      "\n",
      "    accuracy                           0.78       448\n",
      "   macro avg       0.78      0.78      0.78       448\n",
      "weighted avg       0.78      0.78      0.78       448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_targets,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[186  33]\n",
      " [ 67 162]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(test_targets,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At the end of training and testing we can see the model ended up with a roughly 80% accuracy. That means for nearly 3 out of 4 audiobooks customers our model will be able determine if they would be repeat customers and can focus more energy on either enhancing those customers experience or more energy on the those that are predicted to not be return and try to get them to return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
